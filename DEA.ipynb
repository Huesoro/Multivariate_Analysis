{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2023_2024 = pd.read_excel(\"DATOS HISTÓRICOS 2023_2024_TODAS ESTACIONES_ITESM.xlsx\")\n",
    "df2022_2023 = pd.read_excel(\"DATOS HISTÓRICOS 2022_2023_TODAS ESTACIONES.xlsx\", sheet_name=None)\n",
    "df2020_2021 = pd.read_excel(\"DATOS HISTÓRICOS 2020_2021_TODAS ESTACIONES.xlsx\", sheet_name=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1\n",
    "## A) Dimensión de la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtener shape de cada dataframe\n",
    "print(\"Shape de df2023_2024: \", df2023_2024.shape)\n",
    "for key in df2022_2023:\n",
    "    print(\"Shape de df2022_2023: \", key , df2022_2023[key].shape)\n",
    "for key in df2020_2021:\n",
    "    print(\"Shape de df2020_2021: \", key , df2020_2021[key].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2\n",
    "## A) Selecciona el conjunto de datos a utilizar\n",
    "Decide qué conjunto de datos se utilizará. Explica por qué se incluyeron o excluyeron ciertos datos.\n",
    "Identifica las columnas objetivo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bases de datos seleccionadas: Datos historicos 2023-2024, Datos historicos 2022-2023, Datos historicos 2020-2021 y datos históricos 2021.\n",
    "Estas bases se seleccionaron pues son las más recientes, completas y presentan un formato similar para su análisis. Al ser series de tiempo se pueden juntar entre ellas para hacer un análisis más completo.\n",
    "Las columnas objetivo seria tener cada columna con el formato \"Estación Contaminante (medida)\", indexados por su fecha, de manera que se puedan almacenar los datos como una serie de tiempo durante los 4 años."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hacer columnas similares para cada base de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparación de 2023-2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2023_2024.loc[0,:] = df2023_2024.loc[0,:].fillna(\"A\") #Se rellena un valor de A para que no haya problemas al juntar las entradas\n",
    "df2023_2024.columns = df2023_2024.columns.str.split('.').str[0] #Se toma el nombre de la columna sin el .1 o .2\n",
    "df2023_2024 = df2023_2024.replace(\"WDV\", \"WDR\") # Se cambia el nombre de la columna WDV a WDR, puesto que en las demas bases de datos es WDR\n",
    "df2023_2024.columns = (df2023_2024.columns + \" \" + df2023_2024.loc[0] + \" (\" + df2023_2024.loc[1] +\")\").str.upper() #Se junta el nombre de la medida, con la estacion\n",
    "df2023_2024.drop([0,1], inplace=True) # Se eliminan las filas que contenian los nombres de las columnas\n",
    "df2023_2024.rename(columns={\"UNNAMED: 0 A (DATE)\": \"FECHA\"}, inplace=True) #Se cambia el nombre de la columna de fecha\n",
    "df2023_2024[\"FECHA\"] = pd.to_datetime(df2023_2024[\"FECHA\"]) # Se convierte la columna de fecha a datetime\n",
    "#Se cambiar el nombre de las columnas para que coincidan con las demas bases de datos\n",
    "df2023_2024.columns = df2023_2024.columns.str.replace(\"SURESTE2\", \"SURESTE 2\")\n",
    "df2023_2024.columns = df2023_2024.columns.str.replace(\"NORESTE2\", \"NORESTE 2\")\n",
    "df2023_2024.columns = df2023_2024.columns.str.replace(\"SUROESTE2\", \"SUROESTE 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparación de 2022-2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea un diccionario con los nombres de las columnas de df2022_2023\n",
    "modified_keys = {}\n",
    "\n",
    "for key in df2022_2023.keys():\n",
    "    # Se revisa si la ultima letra de la llave es un digito\n",
    "    if key[-1].isdigit():\n",
    "        # Se modifica la llave para que tenga un espacio entre la penultima letra y el ultimo digito\n",
    "        new_key = key[:-1] + \" \" + key[-1]\n",
    "    else:\n",
    "        # De lo contrario, se deja la llave igual\n",
    "        new_key = key\n",
    "    # Y se guarda la llave modificada en el diccionario\n",
    "    modified_keys[key] = new_key\n",
    "\n",
    "# Se cambiar el nombre de las columnas de las bases de datos de 2022_2023 con las llaves generadas\n",
    "df2022_2023 = {modified_keys[key]: df for key, df in df2022_2023.items()}\n",
    "#Se cambian las unidades para que coincidan con las de df2023_2024\n",
    "units = {\"CO\": \"(PPM)\", \"CO2\": \"(PPM)\", \"NO\": \"(PPB)\", \"NO2\": \"(PPB)\", \"NOX\": \"(PPB)\", \"O3\": \"(PPB)\", \"PM10\": \"(UG/M3)\", \"PM2.5\": \"(UG/M3)\", \"SO2\": \"(PPB)\", \"PRS\": \"(MMHG)\", \"RH\": \"(%)\", \"TOUT\": \"(DEGC)\", \"WSP\": \"(M/S)\", \"RAINF\": \"(MM/HR)\", \"WD\": \"(DEG)\", \"WSR\": \"(KMPH)\", \"WDR\": \"(DEG)\", \"SR\": \"(KW/M2)\"}\n",
    "df2022_2023.keys()\n",
    "for key in df2022_2023.keys():\n",
    "    # Se agregan las unidades a las columnas que coinciden con las de df2023_2024 \n",
    "    for column in df2022_2023[key].columns:\n",
    "        if column in units:\n",
    "            df2022_2023[key].rename(columns={column: key+ \" \" + column + \" \" + units[column]}, inplace=True)\n",
    "        \n",
    "\n",
    "#Se junta el diccionario de df2022_2023 en un solo dataframe\n",
    "from functools import reduce\n",
    "\n",
    "# Se elimina catalogo de las bases de datos\n",
    "dfs_to_merge = [df2022_2023[key] for key in df2022_2023 if key != 'CATÁLOGO']\n",
    "\n",
    "# Se juntan las bases de datos por la columna de fecha\n",
    "df2022_2023_merged = reduce(lambda left, right: pd.merge(left, right, on='date',how=\"outer\"), dfs_to_merge)\n",
    "\n",
    "# Se cambia el nombre de la columna de fecha y se cambia el tipo de dato a datetime\n",
    "df2022_2023_merged.rename(columns={\"date\": \"FECHA\"}, inplace=True)\n",
    "df2022_2023_merged[\"FECHA\"] = pd.to_datetime(df2022_2023_merged[\"FECHA\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparación de 2021-2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se crea un diccionario con los nombres de las columnas de df2020_2021\n",
    "modified_keys = {}\n",
    "\n",
    "for key in df2020_2021.keys():\n",
    "    # Se revisa si la ultima letra de la llave es un digito\n",
    "    if key[-1].isdigit():\n",
    "        # Se modifica la llave para que tenga un espacio entre la penultima letra y el ultimo digito\n",
    "        new_key = key[:-1] + \" \" + key[-1]\n",
    "    else:\n",
    "        # De lo contrario, se deja la llave igual\n",
    "        new_key = key\n",
    "    # Y se guarda la llave modificada en el diccionario\n",
    "    modified_keys[key] = new_key\n",
    "\n",
    "# Se cambia el nombre de las columnas de las bases de datos de 2020_2021 con las llaves generadas\n",
    "df2020_2021 = {modified_keys[key]: df for key, df in df2020_2021.items()}\n",
    "\n",
    "for key in df2020_2021.keys():\n",
    "    # Se agregan las unidades a las columnas que coinciden con las de df2023_2024\n",
    "    for column in df2020_2021[key].columns:\n",
    "        if column in units:\n",
    "            df2020_2021[key].rename(columns={column: key + \" \" + column + \" \" + units[column]}, inplace=True)\n",
    "\n",
    "\n",
    "# Se elimina catalogo y noroeste 3 de las bases de datos\n",
    "dfs_to_merge = [df2020_2021[key] for key in df2020_2021 if key not in ['CATÁLOGO', 'NOROESTE 3']]\n",
    "\n",
    "# Merge all DataFrames on the 'date' column\n",
    "df2020_2021_merged = reduce(lambda left, right: pd.merge(left, right, on='date', how=\"outer\"), dfs_to_merge)\n",
    "\n",
    "# Se cambia el nombre de la columna de fecha y se cambia el tipo de dato a datetime\n",
    "df2020_2021_merged.rename(columns={\"date\": \"FECHA\"}, inplace=True)\n",
    "df2020_2021_merged[\"FECHA\"] = pd.to_datetime(df2020_2021_merged[\"FECHA\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimensiones de los datasets\n",
    "print(\"Dimensiones de df2023_2024: \", df2023_2024.shape)\n",
    "print(\"Dimensiones de df2022_2023_merged: \", df2022_2023_merged.shape)\n",
    "print(\"Dimensiones de df2020_2021_merged: \", df2020_2021_merged.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se eliminan las columnas que no tienen al menos el 2% de los datos\n",
    "df2023_2024 = df2023_2024.dropna(thresh=0.02*len(df2023_2024), axis=1)\n",
    "df2022_2023_merged = df2022_2023_merged.dropna(thresh=0.02*len(df2022_2023_merged), axis=1)\n",
    "df2020_2021_merged = df2020_2021_merged.dropna(thresh=0.02*len(df2020_2021_merged), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nuevas dimensiones de los datasets\n",
    "print(\"Dimensiones de df2023_2024: \", df2023_2024.shape)\n",
    "print(\"Dimensiones de df2022_2023_merged: \", df2022_2023_merged.shape)\n",
    "print(\"Dimensiones de df2020_2021_merged: \", df2020_2021_merged.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hacer las fechas datetime\n",
    "df2023_2024[\"FECHA\"] = pd.to_datetime(df2023_2024[\"FECHA\"])\n",
    "df2022_2023_merged[\"FECHA\"] = pd.to_datetime(df2022_2023_merged[\"FECHA\"])\n",
    "df2020_2021_merged[\"FECHA\"] = pd.to_datetime(df2020_2021_merged[\"FECHA\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unir las bases de datos por fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2022_merged = df2022_2023_merged.query(\"FECHA < '2023-01-01'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_to_merge = [df2023_2024, df2022_merged, df2020_2021_merged]\n",
    "\n",
    "# Merge all DataFrames on the 'date' column\n",
    "df_time_serie = pd.concat([dfs_to_merge[0], dfs_to_merge[1], dfs_to_merge[2]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_serie.set_index(\"FECHA\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ordenar por fecha\n",
    "df_time_serie.sort_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the index of the maximun value of each column\n",
    "max_values = df_time_serie.idxmax()\n",
    "max_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se cambian los valores a float\n",
    "df_time_serie = df_time_serie.astype(float)\n",
    "#Se obtine el rango posible de valores por contaminante\n",
    "contaminantes = [\"CO\", \"NO\", \"NO2\", \"NOX\", \"O3\", \"PM10\", \"PM2.5\", \"SO2\", \"RAINF\", \"RH\",\"TOUT\",\"WSR\",\"WDR\", \"SR\", \"PRS\"]\n",
    "for contaminante in contaminantes:\n",
    "    print(contaminante)\n",
    "    print(\"Min: \", df_time_serie.filter(regex=rf\"\\b{contaminante}\\b\").min().min())\n",
    "    print(\"Max: \", df_time_serie.filter(regex=rf\"\\b{contaminante}\\b\").max().max())\n",
    "    #get the index of the max value\n",
    "    print(\"Fecha del maximo: \", max_values.filter(regex=rf\"\\b{contaminante}\\b\").values[0])\n",
    "    #Get the estacion del maximo\n",
    "    print(\"Estacion del maximo: \", max_values.filter(regex=rf\"\\b{contaminante}\\b\").index[0])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_serie = df_time_serie.astype(float)\n",
    "#Se obtine el rango posible de valores por contaminante\n",
    "contaminantes = [\"CO\", \"NO\", \"NO2\", \"NOX\", \"O3\", \"PM10\", \"PM2.5\", \"SO2\", \"RAINF\", \"RH\",\"SR\",\"TOUT\",\"PRS\",\"WSR\",\"WDR\"]\n",
    "for contaminante in contaminantes:\n",
    "    print(contaminante)\n",
    "    print(\"Min: \", df_time_serie.filter(regex=rf\"\\b{contaminante}\\b\").min())\n",
    "    print(\"Max: \", df_time_serie.filter(regex=rf\"\\b{contaminante}\\b\").max())\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Se obtinen los cuantiles de los contaminantes\n",
    "contaminantes = [\"CO\", \"NO\", \"NO2\", \"NOX\", \"O3\", \"PM10\", \"PM2.5\", \"SO2\", \"RAINF\", \"RH\",\"TOUT\",\"WSR\",\"WDR\"]\n",
    "for contaminante in contaminantes:\n",
    "    print(contaminante)\n",
    "    print(df_time_serie.filter(regex=rf\"\\b{contaminante}\\b\").quantile([0.25,0.5,0.75]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se obtiene el menor cuartil de cada contaminante en 0.25, asi como el mayor cualtil de 0.75 de cada contaminante\n",
    "contaminantes = [\"CO\", \"NO\", \"NO2\", \"NOX\", \"O3\", \"PM10\", \"PM2.5\", \"SO2\", \"RAINF\", \"RH\",\"TOUT\",\"WSR\",\"WDR\"]\n",
    "for contaminante in contaminantes:\n",
    "    print(contaminante)\n",
    "    print(\"Cuartil 0.25: \", df_time_serie.filter(regex=rf\"\\b{contaminante}\\b\").quantile(0.25).min())\n",
    "    print(\"Cuartil 0.75: \", df_time_serie.filter(regex=rf\"\\b{contaminante}\\b\").quantile(0.75).max())\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Promedios\n",
    "contaminantes = [\"CO\", \"NO\", \"NO2\", \"NOX\", \"O3\", \"PM10\", \"PM2.5\", \"SO2\", \"RAINF\", \"RH\",\"SR\",\"TOUT\",\"PRS\",\"WSR\",\"WDR\"]\n",
    "for contaminante in contaminantes:\n",
    "    print(contaminante)\n",
    "    promedio = df_time_serie.filter(regex=rf\"\\b{contaminante}\\b\").mean().mean()\n",
    "    print(f\"Promedio: {promedio}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mediana\n",
    "contaminantes = [\"CO\", \"NO\", \"NO2\", \"NOX\", \"O3\", \"PM10\", \"PM2.5\", \"SO2\", \"RAINF\", \"RH\",\"SR\",\"TOUT\",\"PRS\",\"WSR\",\"WDR\"]\n",
    "for contaminante in contaminantes:\n",
    "    print(contaminante)\n",
    "    mediana = df_time_serie.filter(regex=rf\"\\b{contaminante}\\b\").median().max()\n",
    "    print(f\"Mediana: {mediana}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Moda\n",
    "contaminantes = [\"CO\", \"NO\", \"NO2\", \"NOX\", \"O3\", \"PM10\", \"PM2.5\", \"SO2\", \"RAINF\", \"RH\",\"SR\",\"TOUT\",\"PRS\",\"WSR\",\"WDR\"]\n",
    "for contaminante in contaminantes:\n",
    "    print(contaminante)\n",
    "    moda = df_time_serie.filter(regex=rf\"\\b{contaminante}\\b\").mode().max()\n",
    "    print(f\"Moda: {moda}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtener la desviacion estandar de cada contaminante\n",
    "contaminantes = [\"CO\", \"NO\", \"NO2\", \"NOX\", \"O3\", \"PM10\", \"PM2.5\", \"SO2\", \"RAINF\", \"RH\",\"SR\",\"TOUT\",\"PRS\",\"WSR\",\"WDR\"]\n",
    "for contaminante in contaminantes:\n",
    "    print(contaminante)\n",
    "    print(df_time_serie.filter(regex=rf\"\\b{contaminante}\\b\").std().max())\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varianza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtener la varianza de cada contaminante\n",
    "contaminantes = [\"CO\", \"NO\", \"NO2\", \"NOX\", \"O3\", \"PM10\", \"PM2.5\", \"SO2\", \"RAINF\", \"RH\",\"SR\",\"TOUT\",\"PRS\",\"WSR\",\"WDR\"]\n",
    "for contaminante in contaminantes:\n",
    "    print(contaminante)\n",
    "    print(df_time_serie.filter(regex=rf\"\\b{contaminante}\\b\").std().max()**2)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2) Verifica la calidad de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_time_serie_filtered = df_time_serie.copy()\n",
    "#Eliminar las columnas de NOROESTE 3\n",
    "df_time_serie_filtered = df_time_serie_filtered.drop(columns=df_time_serie_filtered.filter(regex=\"NOROESTE 3\").columns)\n",
    "df_time_serie_filtered.replace(-9999, np.nan, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se limpia el valor extremo de la columna de TOUT menor a -20\n",
    "df_tout_columns = df_time_serie_filtered.filter(regex=rf\"\\bTOUT\\b\").columns\n",
    "df_time_serie_filtered[df_tout_columns] = df_time_serie_filtered[df_tout_columns].mask(df_time_serie_filtered[df_tout_columns].lt(-20), np.nan)\n",
    "\n",
    "# Se limpia el valor extremo de la columna de CO menor a 0\n",
    "df_CO_columns = df_time_serie_filtered.filter(regex=rf\"\\bCO\\b\").columns\n",
    "df_time_serie_filtered[df_CO_columns] = df_time_serie_filtered[df_CO_columns].mask(df_time_serie_filtered[df_CO_columns].lt(0), np.nan)\n",
    "\n",
    "# Se limpia el valor extremo de la columna de NO2 menor a 0\n",
    "df_NO_columns = df_time_serie_filtered.filter(regex=rf\"\\bNO2\\b\").columns\n",
    "df_time_serie_filtered[df_NO_columns] = df_time_serie_filtered[df_NO_columns].mask(df_time_serie_filtered[df_NO_columns].lt(0), np.nan)\n",
    "\n",
    "# Se limpia el valor extremo de la columna de NO2 menor a 0\n",
    "df_SR_columns = df_time_serie_filtered.filter(regex=rf\"\\bSR\\b\").columns\n",
    "df_time_serie_filtered[df_SR_columns] = df_time_serie_filtered[df_SR_columns].mask(df_time_serie_filtered[df_SR_columns].lt(0), np.nan)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hacer función para reemplazar valor mayores a cierta cantidad, dado un contaminante y un dataframe\n",
    "def replace_outliers(contaminante,cantidad, df):\n",
    "    df_columns = df.filter(regex=rf\"\\b{contaminante}\\b\").columns\n",
    "    df[df_columns] = df[df_columns].mask(df[df_columns].gt(cantidad), np.nan)\n",
    "\n",
    "def max_std(contaminante, df):\n",
    "    return df.filter(regex=rf\"\\b{contaminante}\\b\").std().max() * 3\n",
    "# moda = df_time_serie.filter(regex=rf\"\\b{contaminante}\\b\").std().max()\n",
    "contaminantes = [\"CO\", \"NO\", \"NO2\", \"NOX\", \"O3\", \"PM10\", \"PM2.5\", \"SO2\"]\n",
    "    \n",
    "for contaminante in contaminantes:\n",
    "    replace_outliers(contaminante, max_std(contaminante, df_time_serie_filtered), df_time_serie_filtered)\n",
    "\n",
    "replace_outliers(\"RAINF\", 800, df_time_serie)\n",
    "replace_outliers(\"RH\", 30, df_time_serie)\n",
    "replace_outliers(\"SR\", 5, df_time_serie)\n",
    "replace_outliers(\"TOUT\", 60, df_time_serie)\n",
    "replace_outliers(\"PRS\", 900, df_time_serie)\n",
    "replace_outliers(\"WSR\", 300, df_time_serie)\n",
    "replace_outliers(\"WDR\", 360, df_time_serie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_serie_2022 = df_time_serie_filtered.query(\"FECHA >= '2022-01-01'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graficos sin outliers raros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CO_columns = df_time_serie_2022.filter(regex=rf\"\\bPM10\\b\").columns.tolist()\n",
    "sns.set(style=\"whitegrid\", palette=\"muted\")\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "sns.boxplot(data=df_time_serie_2022[CO_columns], ax=ax)\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=85)\n",
    "ax.set_title(\"PM10 Levels Across Different Regions\", fontsize=16, weight='bold')\n",
    "ax.set_ylabel(\"PM10 Levels (PPM)\", fontsize=14)\n",
    "ax.set_xlabel(\"Regions\", fontsize=14)\n",
    "sns.despine(trim=True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"PM10_levels.png\", dpi = 400)\n",
    "plt.show\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplots\n",
    "CO_columns = df_time_serie_2022.filter(regex=rf\"\\bCO\\b\").columns.tolist()\n",
    "df_time_serie_2022.boxplot(column=CO_columns, figsize=(15, 10), rot=85)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se hace un histograma por contaminante agrupado por estación\n",
    "for contaminante in contaminantes:\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    df_time_serie_2022.filter(regex=rf\"\\b{contaminante}\\b\").hist(ax=ax, bins=40)\n",
    "    plt.title(contaminante)\n",
    "    plt.savefig(f\"{contaminante}_histogram.png\", dpi = 400)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "estaciones = ['SURESTE', 'NORESTE', 'CENTRO', 'NOROESTE', 'SUROESTE', 'NORTE', 'SUR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "# Quitar la palabra 'SURESTE 2' de cada histograma individual\n",
    "\n",
    "df_time_serie_2022.filter(regex=rf\"\\b{'SURESTE 2'}\\b\").hist(ax=ax, bins=40)\n",
    "plt.title('SURESTE 2')\n",
    "plt.savefig(f\"{'SURESTE 2'}_histogram.png\", dpi = 400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se hace un histograma por contaminante agrupado por estación\n",
    "for estacion in estaciones:\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    df_time_serie_2022.filter(regex=rf\"\\b{estacion}\\b\").hist(ax=ax, bins=40)\n",
    "    plt.title(estacion)\n",
    "    plt.savefig(f\"{estacion}_histogram.png\", dpi = 400)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtener un histograma de los contaminantes en cada estación\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.histplot(data=df_time_serie_2022.filter(regex=rf\"\\bSURESTE\\b\"), bins=40)\n",
    "plt.title(\"SURESTE\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heat map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(df_time_serie_2022.filter(regex=rf\"\\bPM10\\b\").corr('spearman'), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"PM2.5_heatmap.png\", dpi = 400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "sns.heatmap(df_time_serie_2022.filter(regex=\"SUROESTE 2\").corr('spearman'), annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"PM2.5_heatmap.png\", dpi = 400)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the entry filled with NaNs\n",
    "df_time_serie_2022.loc['2023-02-11 19:00:00'] = np.nan\n",
    "df_time_serie_2022.loc['2023-10-31 17:00:00'] = np.nan\n",
    "#Set the index to datetime\n",
    "df_time_serie_2022.index = pd.to_datetime(df_time_serie_2022.index) \n",
    "# Mes\n",
    "df_time_serie_2022[\"Mes\"] = df_time_serie_2022.index.month\n",
    "#Day of year\n",
    "df_time_serie_2022[\"Dia\"] = df_time_serie_2022.index.dayofyear\n",
    "#Hour\n",
    "df_time_serie_2022[\"Hora\"] = df_time_serie_2022.index.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_series_2022 = df_time_serie_2022.asfreq('h',fill_value=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=7, weights=\"distance\")\n",
    "df_time_serie_2022_nonans = imputer.fit_transform(df_time_serie_2022)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the numpy array back to a DataFrame with the same columns\n",
    "\n",
    "df_time_serie_2022_nonans = pd.DataFrame(df_time_serie_2022_nonans, columns=df_time_serie_2022.columns, index=df_time_serie_2022.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time_serie_2022_nonans.to_csv(\"df_time_serie_2022.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
